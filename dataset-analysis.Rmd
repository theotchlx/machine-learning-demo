---
title: "Wine dataset analysis"
author: "BROINE Thomas, BRONSIN Baptiste, TCHILINGUIRIAN Théo"
date: "2025-02-17"
output: html_document
---

<style type="text/css">

body, td {
   font-size: 16px;
}
code.r{
  font-size: 14px;
}
pre {
  font-size: 14px
}
</style>

```{r setup, include=FALSE}
library(readr)
library(knitr)
library(dplyr)
library(caret)
library(randomForest)
library(corrplot)
library(rpart)
```

## Dataset analysis

We observe that every variable is quantitative.

- **fixed acidity** g(tartaric acid)/dm^3
- **volatile acidity** g(acetic acid)/dm^3
- **Citric acid** g/dm^3
- **Residual sugar** g/dm^3
- **Chlorides** g(sodium chloride)/dm^3
- **Free sulfur dioxide** mg/dm^3
- **Total sulfur dioxide** mg/dm^3
- **Density** g/cm^3
- **pH**
- **Sulphates** g(potassium sulphate)/dm^3
- **Alcohol** vol.%

## Quick summary of the data

```{r dataset_analysis, echo=FALSE}
# Charger le jeu de données
winequality_white <- read_delim("dataset/winequality-white.csv", delim = ";", escape_double = FALSE, trim_ws = TRUE, show_col_types = FALSE)

# Normaliser les noms des colonnes
colnames(winequality_white) <- make.names(colnames(winequality_white))

# Afficher les dernières lignes du jeu de données
kable(tail(winequality_white))

summary(winequality_white)
```

### Histograms on important variables

Here is an overview of some important variables that will be used for the analysis.

```{r histograms, fig.width=7, fig.height=7, echo=FALSE}
# Exemple d'histogrammes pour quelques variables
par(mfrow=c(2,2))  # pour afficher plusieurs graphes sur une grille 2x2

hist(winequality_white$alcohol, 
     main="Histogram: alcohol", col="skyblue", xlab="Alcohol ( vol.% )")
hist(winequality_white$density,
     main="Histogram: density", col="skyblue", xlab="Density ( g/cm^3 )")
hist(winequality_white$pH,
     main="Histogram: pH", col="skyblue", xlab="pH")
hist(winequality_white$quality, 
     main="Histogram: quality", col="skyblue", xlab="Quality")
```

These histograms display the quantities of some important variables, per their values.

### Boxplots on important variables

The following boxplots show the distribution of some important variables, per the wine quality.

#### Alcohol content

```{r boxplot_alcohol, echo=FALSE}
# Relations entre qualité et alcool.
boxplot(alcohol ~ quality, data=winequality_white,
        main="Alcohol content according to wine quality",
        xlab="Quality", ylab="Alcohol content")
```

This boxplot shows that generally, higher wine qualities have higher alcohol content. The best quality of wine has a consistant high value in alcohol content.  
However, it is important to note that there are very few data on higher quality wine (as can be seen in the above histograms).

#### Density

```{r boxplot_density, echo=FALSE}
# Relations entre qualité et densité
boxplot(density ~ quality, data=winequality_white, ylim=c(0.98, 1.01),
        main="Density according to wine quality",
        xlab="Quality", col="skyblue", ylab="Density")
```

This boxplot shows that higher wine qualities have lower density (that is, different contents such as sugars, alcohol, and other chemicals). The best quality of wine has a consistant low density.

### Correlation

This correlation plot shows the correlation between variables.

```{r correlation, echo=FALSE}
# Calculer la matrice de corrélation
correlation_matrix <- cor(winequality_white)

# Visualiser la matrice de corrélation
corrplot(correlation_matrix, method = "circle")
```

#### Interpreting the results:

This correlation plot shows clearly the correlations between variables.

- A positive correlation (shades of blue) means that if one the variables' value increases, the other increases.
- A negative correlation (shades of red) means that if one of the variables' value increases, the other decreases.

Obviously, wine density is inversely correlated to alcohol content. This can be easily understood when comparing the density of ethanol to that of water:

- Ethanol density : 0.7892 g/mL at 20°C.
- Water density : 0.9982 g/mL at 20°C.

And indeed, if alcohol content is higher, the density lowers, which explains the strong negative correlation.

Other obvious correlations include acidity and pH, density and residual sugar content

Basically, the density is explained by the wine's contents (alcohol, sugars, sulphates, chlorides). Thankfully for this small dataset, the correlation between these variables will not influence the wine quality prediction. For this small dataset, we don't need to eliminate variables for predictions.

## Logistic binary regression

Now, we will do a binary logistic regression for predictive analysis.

First, we classify the quality as binary.

```{r}
winequality_white$quality_binary <- ifelse(winequality_white$quality <= 5, "bad", "good")

# We want a factor (category)
winequality_white$quality_binary <- as.factor(winequality_white$quality_binary)
```

```{r}
# Separate the data into training and test sets
set.seed(123)
trainIndex <- createDataPartition(winequality_white$quality, p = .8,
                                  list = FALSE,
                                  times = 1)
wineTrain <- winequality_white[ trainIndex,]
wineTest  <- winequality_white[-trainIndex,]

wineTrain_withoutQuality <- wineTrain[, !(names(wineTrain) %in% c("quality"))]
wineTrain_withoutQualityBinary <- wineTrain[, !(names(wineTrain) %in% c("quality_binary"))]

wineTest_withoutQuality <- wineTest[, !(names(wineTest) %in% c("quality"))]
wineTest_withoutQualityBinary <- wineTest[, !(names(wineTest) %in% c("quality_binary"))]
wineTest_WithoutAllQuality <- wineTest[, !(names(wineTest) %in% c("quality", "quality_binary"))]

dim(wineTrain)
dim(wineTest)
```

```{r logistic_binary_regression}
# Binary logistic regression
logit_model <- glm(quality_binary ~ fixed.acidity + volatile.acidity + 
                                     citric.acid + residual.sugar + chlorides + 
                                     free.sulfur.dioxide + total.sulfur.dioxide + 
                                     density + pH + sulphates + alcohol,
                   data = wineTrain_withoutQuality, # We need here to predict the binary quality
                   family = binomial)

# Résumé du modèle
summary(logit_model)

# Prédiction sur l'ensemble de test (probabilité d'être "bon")
predict_prob_logit <- predict(logit_model, newdata = wineTest_WithoutAllQuality, type = "response") # We remove all quality clues

# Pour obtenir la classe prédite (0/1) selon un cutoff=0.5
predict_class_logit <- ifelse(predict_prob_logit <= 0.5, "bad", "good")
predict_class_logit <- factor(predict_class_logit, levels = c("bad", "good"))

# Table de confusion
conf_mat_logit <- table(Predicted = predict_class_logit, 
                        Actual = wineTest$quality_binary)
conf_mat_logit

# Accuracy
accuracy_logit <- sum(diag(conf_mat_logit)) / sum(conf_mat_logit)
accuracy_logit
```

## Decision Tree Model
```{r}
# Entraîner le modèle avec un seul arbre de décision
tree_model <- rpart(quality ~ ., data = wineTrain_withoutQualityBinary, method = "class") # We need here to predict the quality

# Afficher l'arbre de décision
print(tree_model)

# Faire des prédictions sur l'ensemble de test
tree_predictions <- predict(tree_model, wineTest_WithoutAllQuality, type = "class") # We remove all quality clues

# Afficher les prédictions
head(tree_predictions)

# Calculer la précision
tree_accuracy <- sum(tree_predictions == wineTest$quality) / nrow(wineTest)

# Afficher la précision
print(paste("Précision :", round(tree_accuracy * 100, 2), "%"))
```

There are two types of log loss calculations: one for binary classification problems and one for multi-class classification problems. In this case, we have a binary classification problem, so we will use the log loss formula for binary classification.

```{r decision-tree-logloss}
y_true <- ifelse(wineTest$quality_binary == "good", 1, 0)

# Convertir les facteurs en numériques
tree_predictions_numeric <- as.numeric(as.character(tree_predictions))

# Convertir les scores de qualité en probabilités
probabilities <- tree_predictions_numeric / 10

# Fonction pour calculer le log loss
log_loss <- function(y_true, probabilities, eps=1e-15){
  probabilities <- pmin(pmax(probabilities, eps), 1 - eps)
  -mean(y_true * log(probabilities) + (1 - y_true) * log(1 - probabilities))
}

# Calcul de la log loss
ll_rf <- log_loss(y_true, probabilities)
print(paste("Log loss :", ll_rf))
```

We also calculate the Brier score of the model.
```{r brier_tree}
# Calculer Brier score pour le modèle decision tree
tree_probabilities <- predict(tree_model, wineTest_WithoutAllQuality)
brier_score_tree <- mean((tree_probabilities[, 2] - y_true)^2)
print(paste("Brier Score (Decision Tree):", round(brier_score_tree, 4)))
```

## Random Forest Model
```{r}
# Entraîner le modèle de forêts aléatoires
randomForestModel <- randomForest(quality ~ ., data = wineTrain_withoutQualityBinary)

# Prédire sur l'ensemble de test
predictions <- predict(randomForestModel, wineTest_WithoutAllQuality)

# Arrondir les prédictions pour obtenir des nombres entiers
rounded_predictions <- round(predictions)

# Afficher les prédictions
head(rounded_predictions)

# Évaluer le modèle
eval <- postResample(pred = rounded_predictions, obs = wineTest$quality)

# Afficher les métriques d'évaluation
eval
```

### RMSE

The RMSE measures the mean difference between the values predicted by the model and the actual values. A lower RMSE value indicates a better fit of the model to the data. Here, the value is `r round(eval[“RMSE”], 4)`, which means that, on average, model predictions deviate from actual values by `r round(eval[“RMSE”], 4)` units.

### Rsquared

The coefficient of determination, or R-squared, represents the proportion of the variance of the dependent variable that can be predicted from the independent variables. An R-squared of 1 means that the model perfectly explains the variance of the data, while an R-squared of 0 means that the model explains no variance. Here, the value is `r round(eval[“Rsquared”], 4)`, indicating that the model explains around `r round(eval[“Rsquared”] * 100, 2)`% of the variance in wine quality.

### MAE

MAE measures the mean absolute error between predictions and actual values. Unlike the RMSE, it does not penalize large errors as much. Here, the value is `r round(eval[“MAE”], 4)`, which means that, on average, model predictions deviate from actual values by `r round(eval[“MAE”], 4)` units.

- Variable importance**: We examined the importance of the different variables in the model. Variables with high importance scores contribute more to the predictions.
- **Visualization**: The variable importance graph allows us to quickly see which characteristics are most influential in predicting wine quality.

```{r}
# Calculer la précision
forest_accuracy <- sum(rounded_predictions == wineTest$quality) / nrow(wineTest)

# Afficher la précision
print(paste("Précision :", round(forest_accuracy * 100, 2), "%"))

# Importer les variables importantes
importance(randomForestModel)
varImpPlot(randomForestModel)
mtext("Importance of variables in the model", side = 1, line = 2, cex = 1.2)
```

In this section, we have trained a random forest model to predict wine quality.

```{r metrics-logloss}
# Convertir les scores de qualité en probabilités
probabilities <- rounded_predictions / 10

# Calcul de la log loss
ll_rf_forest <- log_loss(y_true, probabilities)
print(paste("Log loss :", ll_rf_forest))
```

Binary log loss is a performance measure used to assess the accuracy of the probabilistic predictions of a binary classification model, by comparing the predicted probabilities with the true class (0 or 1) of each sample.

And we calculate the Brier score as well.
```{r brier_forest}
# Calculer Brier score pour le modèle random forest
#...
```


## Interpretation and Comparison

First, we examined the dataset, its variables and their relative importance for our predictions.  
We have done predictions using two models: a single decision tree, and a random forest model.  
To conclude this report, we will now compare their accuracy metrics to determine which model was the better prediction model.

These metrics help us evaluate the performance of our models, by comparing the accuracy, log loss, and Brier score between the two models.

### Accuracy Comparison

The accuracy metric represents the proportion of correctly classified wine quality labels in our test set. We learnt that decision trees are a simple model that is able to capture basic patterns in the data, but are more prone to overfitting when compared to the random forest model, which aggregates multiple trees, and as such improves accuracy by reducing variance.

From our results:

- Decision tree model accuracy: `r tree_accuracy`  
- Random forest model accuracy: `r forest_accuracy`

As we can see, the random forest outperforms the single decision tree in accuracy. In the forest, multiple trees contribute to the final prediction, and in this prediction, this led to a better result in accuracy.

### Log Loss Comparison

The log loss is another performance metric. It measures the uncertainty of a probabilistic classification model.

From our results:

- Decision tree model log loss: `r ll_rf`  
- Random forest model log loss: `r ll_rf_forest`

A lower log loss value indicates that the predicted probabilities are closer to the true values, which is better (the model is better calibrated).

### Brier Score Comparison

The Brier score measures the mean squared error between predicted probabilities and actual outcomes. A lower Brier score indicates better probabilistic predictions.

From our results:

- Decision tree model Brier score: `r brier_score_tree`  
- Random forest model Brier score: Not done.

### Final results

Comparing the performance metrics we were able to calculate shows that the random forest model seemingly outperforms the single decision tree, across multiple evaluation metrics.

We can conclude that the decision tree is a simple model which we can understand more easily, but by combining multiple trees, the random forest model mitigates overfitting and provides more accurate predictions.

We tried to predict wine quality using the given dataset, comparing two prediction models on multiple accuracy metrics, and we demonstrated that the random forest model comes out on top in performance.
